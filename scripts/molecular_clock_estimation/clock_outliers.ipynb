{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import os, subprocess, dendropy\n",
    "from Bio import SeqIO\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set subtype and segment\n",
    "subtype = \"H3N2\"\n",
    "segment = \"PA\"\n",
    "timeperiod = \"0019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = f\"/Users/annelies/Desktop/flu-evolution/{subtype}_{segment}_gisaid_sequences\"\n",
    "raw_fasta = os.path.join(main_dir, \"raw\", f\"{subtype}_{segment}_gisaid_raw.fasta\")\n",
    "metadata_file = os.path.join(main_dir, \"raw\", f\"{subtype}_{segment}_metadata_gisaid_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if raw exists and else create to complete raw from where direct gisaid download is stored\n",
    "if not os.path.isfile(raw_fasta):\n",
    "    gisaid_download_dir = os.path.join(main_dir, \"..\", \"gisaid_raw\", f\"{subtype}_{segment}_{timeperiod}\")\n",
    "    records = []\n",
    "    for f in os.listdir(gisaid_download_dir):\n",
    "        if f.endswith(\".fasta\"):\n",
    "            for r in SeqIO.parse(os.path.join(gisaid_download_dir, f), \"fasta\"):\n",
    "                records.append(r)\n",
    "        elif f.endswith(\".xls\"):\n",
    "            try:\n",
    "                df = pd.concat([df, pd.read_excel(os.path.join(gisaid_download_dir, f))])\n",
    "            except:  \n",
    "                df = pd.read_excel(os.path.join(gisaid_download_dir, f))\n",
    "\n",
    "    with open(raw_fasta, \"w\") as fw:\n",
    "        SeqIO.write(records, fw, \"fasta\")\n",
    "\n",
    "    df.to_csv(metadata_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set files and directory paths\n",
    "cluster_dir = os.path.join(main_dir,\"cluster_seqs\")\n",
    "\n",
    "alignment_dir = os.path.join(main_dir, \"alignment\")\n",
    "tree_dir = os.path.join(main_dir, \"tree\")\n",
    "\n",
    "for d in [cluster_dir, alignment_dir, tree_dir]:\n",
    "    if not os.path.isdir(d):\n",
    "        os.mkdir(d)\n",
    "\n",
    "clean_fasta = os.path.join(main_dir, \"outliers_removed\", f\"{subtype}_{segment}_gisaid_raw.fasta\")\n",
    "\n",
    "molecular_clock_outliers = f\"../data/to_drop/{subtype}_{segment}_gitr.csv\"\n",
    "\n",
    "clean_metadata = os.path.join(main_dir, \"outliers_removed\", f\"{subtype}_{segment}_metadata_gisaid_{timeperiod}.xlsx\")\n",
    "\n",
    "cluster_results = os.path.join(main_dir, \"cdhit_output\", f\"{subtype}_{segment}_clus.clstr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove low quality and max ambig sequences\n",
    "redo = True\n",
    "\n",
    "mco = [] #molecular clock outliers\n",
    "to_remove = []\n",
    "if os.path.isfile(molecular_clock_outliers):\n",
    "    with open(molecular_clock_outliers, \"r\") as fr:\n",
    "        for l in fr:\n",
    "            if \"reason\" in l:\n",
    "                continue\n",
    "            mco.append(l.strip(\"\\n\").split(\",\"))\n",
    "            to_remove.append(l.strip(\"\\n\").split(\",\")[0])\n",
    "\n",
    "if not os.path.isfile(clean_fasta) or redo==True:\n",
    "    records = []\n",
    "    for r in SeqIO.parse(raw_fasta, \"fasta\"):\n",
    "        if r.id.split(\"|\")[0] not in to_remove:\n",
    "            if not check_sequence_length(segment, str(r.seq), 0.95): #check length\n",
    "                mco.append([r.id.split(\"|\")[0], \"too short\"])\n",
    "            elif not check_max_ambig(str(r.seq), 0.01):\n",
    "                mco.append([r.id.split(\"|\")[0], \"too many ambiguous nucleotides\"])\n",
    "            else:\n",
    "                records.append(r)\n",
    "\n",
    "    with open(clean_fasta, \"w\") as fw:\n",
    "        SeqIO.write(records,fw,\"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/nj8tf3tj6gv6m18wpfp2c96m0000gn/T/ipykernel_21333/3057046506.py:2: DtypeWarning: Columns (21,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(metadata_file)\n"
     ]
    }
   ],
   "source": [
    "#filter outliers from metadata\n",
    "metadata = pd.read_csv(metadata_file)\n",
    "bad_ids = [l[0] for l in mco] \n",
    "metadata = metadata[~metadata[\"Isolate_Id\"].isin(bad_ids)]\n",
    "metadata.to_excel(clean_metadata, index=\"False\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CD-HIT\n",
    "Run CD-hit manually on the command line with the following command:  \n",
    "`cd-hit -i {clean_fasta} -o {cluster_results} -c 0.998 -t {number of threads}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse cd hit file\n",
    "with open(cluster_results, \"r\") as f:\n",
    "    lines = [l.strip(\"\\n\") for l in f]\n",
    "\n",
    "clusters = {}\n",
    "for l in lines:\n",
    "    if l.startswith(\">\"):\n",
    "        c = l.lstrip(\">\")\n",
    "        clusters[c] = []\n",
    "    else:\n",
    "        sid = l.split(\">\")[-1].split(\"|\")[0]\n",
    "        clusters[c].append(sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/nj8tf3tj6gv6m18wpfp2c96m0000gn/T/ipykernel_21333/3435786036.py:2: DtypeWarning: Columns (21,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(metadata_file)\n"
     ]
    }
   ],
   "source": [
    "#get dates and set season definitions\n",
    "metadata = pd.read_csv(metadata_file)\n",
    "metadata[\"Collection_Date\"] = pd.to_datetime(metadata[\"Collection_Date\"],format=\"%Y-%m-%d\" )\n",
    "#get season definitions > doing this manually\n",
    "seasons = {\"0001\":[datetime(2000,5,1), datetime(2001,4,30)], \"0102\":[datetime(2001,5,1), datetime(2002,4,30)], \"0203\":[datetime(2002,5,1), datetime(2003,4,30)],\n",
    "           \"0304\":[datetime(2003,5,1), datetime(2004,4,30)], \"0405\":[datetime(2004,5,1), datetime(2005,4,30)], \"0506\":[datetime(2005,5,1), datetime(2006,4,30)],\n",
    "           \"0607\":[datetime(2006,5,1), datetime(2007,4,30)], \"0708\":[datetime(2007,5,1), datetime(2008,4,30)], \"0809\":[datetime(2008,5,1), datetime(2009,4,30)],\n",
    "           \"0910\":[datetime(2009,5,1), datetime(2010,4,30)], \"1011\":[datetime(2010,5,1), datetime(2011,4,30)], \"1112\":[datetime(2011,5,1), datetime(2012,4,30)],\n",
    "           \"1213\":[datetime(2012,5,1), datetime(2013,4,30)], \"1314\":[datetime(2013,5,1), datetime(2014,4,30)], \"1415\":[datetime(2014,5,1), datetime(2015,4,30)], \n",
    "           \"1516\":[datetime(2015,5,1), datetime(2016,4,30)], \"1617\":[datetime(2016,5,1), datetime(2017,4,30)], \"1718\":[datetime(2017,5,1), datetime(2018,4,30)],\n",
    "           \"1819\":[datetime(2018,5,1), datetime(2019,4,30)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split clusters per season\n",
    "def determine_season(d):\n",
    "    \"\"\"\n",
    "    determine the flu seasons based on the date\n",
    "    \"\"\"\n",
    "    ss = list(seasons.keys())\n",
    "    season_starts = [v[0] for v  in seasons.values()]\n",
    "    season_ends = [v[-1] for v in seasons.values()]\n",
    "\n",
    "    if d > season_ends[-1] or d < season_starts[0]:\n",
    "        return None\n",
    "\n",
    "    for i, se in enumerate(season_ends):\n",
    "        if d < se:\n",
    "            return ss[i]\n",
    "\n",
    "clus_representatives = {s:{} for s in seasons.keys()}#seq to select per season\n",
    "season_clus_ids= {s:{} for s in seasons.keys()}#seq to select per season\n",
    "\n",
    "singles = {s:[] for s in seasons.keys()}\n",
    "\n",
    "for c, ids in clusters.items():\n",
    "\n",
    "    if len(ids)==1:\n",
    "        d = str(np.datetime_as_string(metadata[metadata[\"Isolate_Id\"]==ids[0]][\"Collection_Date\"].values[0]))\n",
    "        d = datetime(int(d.split(\"-\")[0]), int(d.split(\"-\")[1]), int(d.split(\"-\")[2].split(\"T\")[0]))\n",
    "        s = determine_season(d) \n",
    "        if s:\n",
    "            singles[s].append(ids[0])\n",
    "        continue\n",
    "\n",
    "    dates = [str(np.datetime_as_string(metadata[metadata[\"Isolate_Id\"]==sid][\"Collection_Date\"].values[0])) for sid in ids]\n",
    "    dates = [datetime(int(d.split(\"-\")[0]), int(d.split(\"-\")[1]), int(d.split(\"-\")[2].split(\"T\")[0])) for d in dates]\n",
    "    \n",
    "    csons = [determine_season(d) for d in dates]\n",
    "    ids = [sid for i, sid in enumerate(ids) if csons[i]!=None]\n",
    "    dates = [d for i,d in enumerate(dates) if csons[i]!=None]\n",
    "    csons = [cson for cson in csons if cson!=None]\n",
    "\n",
    "\n",
    "    #if all seqs in one season select min and max date\n",
    "    for cson in set(csons):\n",
    "        cson_ids = [ids[i] for i, x in enumerate(csons) if x == cson]\n",
    "        cson_dates = [dates[i] for i, x in enumerate(csons) if x == cson]\n",
    "            \n",
    "        eldest = cson_ids[cson_dates.index(min(cson_dates))]\n",
    "        youngest = cson_ids[cson_dates.index(max(cson_dates))]\n",
    "\n",
    "        season_clus_ids[cson][c] =  cson_ids\n",
    "\n",
    "        clus_representatives[cson][c] = []\n",
    "        if eldest not in clus_representatives[cson][c]:\n",
    "            clus_representatives[cson][c].append(eldest)\n",
    "        if youngest not in clus_representatives[cson][c]:\n",
    "            clus_representatives[cson][c].append(youngest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sequence files per batch\n",
    "#season_sets = [[\"0001\",\"0102\",\"0203\", \"0304\", \"0405\", \"0506\", \"0607\", \"0708\", \"0809\", \"0910\"],[\"1011\",\"1112\", \"1213\", \"1314\",\"1415\"], [\"1516\", \"1617\", \"1718\", \"1819\"]] #H3N2\n",
    "season_sets = [[\"0910\", \"1011\", \"1112\", \"1213\", \"1314\", \"1415\"],[\"1516\", \"1617\", \"1718\",\"1819\"]] #H1N1pdm\n",
    "\n",
    "for s_set in season_sets:\n",
    "    ids = []\n",
    "    for s in s_set:\n",
    "        for l in clus_representatives[s].values():\n",
    "            ids.extend(l)\n",
    "\n",
    "        ids.extend(singles[s])\n",
    "\n",
    "    records = []\n",
    "    for r in SeqIO.parse(clean_fasta, \"fasta\"):\n",
    "        if r.id.split(\"|\")[0] in ids:\n",
    "            for i in [\":\", \",\", \"(\", \")\", \"'\"]:\n",
    "                if i in r.id:\n",
    "                    r.id = r.id.replace(i, \"\")\n",
    "                    r.name = r.name.replace(i, \"\")\n",
    "                    r.description = r.description.replace(i, \"\")\n",
    "            records.append(r)\n",
    "\n",
    "    out_fasta = f\"{subtype}_{segment}_gisaid_{s_set[0]}_{s_set[-1]}.fasta\"\n",
    "    with open(os.path.join(cluster_dir, out_fasta), \"w\") as fw:\n",
    "       SeqIO.write(records,fw,\"fasta\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct MSA\n",
    "MSA via MAFFt with following command `mafft --auto --thread 3 --keeplength --addfragments {cluster_file} {reference} > {alignment_file}`\n",
    "removing reference afterwards > as reference used in from 1968 and will f*** with molecular clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove reference from aligment\n",
    "for f in os.listdir(alignment_dir):\n",
    "    if not f.startswith(\".\"):\n",
    "        ff = os.path.join(alignment_dir,f)\n",
    "\n",
    "        records = list(SeqIO.parse(ff, \"fasta\"))\n",
    "        if \"-ref\" in records[0].id:\n",
    "            with open(ff,\"w\")as fw:\n",
    "                SeqIO.write(records[1:],fw,\"fasta\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct tree\n",
    "Don't need exact precision and want trees as fast as possible, so therefore using fasttree  \n",
    "command used `fasttree -gtr -nt {alignment} > {tree}`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tempest \n",
    "getting dates for tempest > takes dates in order of tree  \n",
    "load annotated trees in tempest > best-fitting root > root-to-tip and residuals to find the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotate tree files with dates\n",
    "for f in os.listdir(tree_dir):\n",
    "    labels = []\n",
    "    if f.endswith(\".tree\") and \"annotated\" not in f:\n",
    "        tree = dendropy.Tree.get(path=os.path.join(tree_dir,f), schema=\"newick\")\n",
    "        for l in tree.leaf_node_iter():\n",
    "            labels.append(l.taxon.label.replace(\" \", \"_\"))\n",
    "\n",
    "        label_dates = {}\n",
    "        for label in labels:\n",
    "            rid = label.split(\"|\")[0]\n",
    "            d = metadata[metadata[\"Isolate_Id\"]==rid][\"Collection_Date\"].values[0].astype(str).split(\"T\")[0]\n",
    "            label_dates[label] = f\"{label}|{d}\"\n",
    "\n",
    "        with open(os.path.join(tree_dir,f), \"r\") as fr:\n",
    "            tree_line = fr.readline()\n",
    "\n",
    "        for label, new_label in label_dates.items():\n",
    "            tree_line  = tree_line.replace(label, new_label)\n",
    "\n",
    "        with open(os.path.join(tree_dir,f.replace(\".tree\", \"_annotated.tree\")), \"w\") as fw:\n",
    "            fw.write(tree_line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing problematic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempest_outliers = [\"EPI_ISL_286128\", \"EPI_ISL_498990\", \"EPI_ISL_309766\", \"EPI_ISL_498987\",\"EPI_ISL_12995401\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make reverse list of clusters\n",
    "reversed_clusters = {c:k for k,v in clusters.items() for c in v}\n",
    "\n",
    "#clus_representatives_reversed = {i:f\"{c}-{s}\" for s, d in clus_representatives.items() for c, v in d.items() for i in  v}\n",
    "season_clus_ids_reversed = {i:f\"{c}-{s}\" for s, d in season_clus_ids.items() for c, v in d.items() for i in  v}\n",
    "\n",
    "for outlier in tempest_outliers:\n",
    "    \n",
    "    if outlier in season_clus_ids_reversed.keys(): #else outlier is already removed\n",
    "        clus, cson = season_clus_ids_reversed[outlier].split(\"-\")\n",
    "        cluster_season_ids = season_clus_ids[cson][clus]\n",
    "        \n",
    "        for cid in cluster_season_ids:\n",
    "            l = [cid, \"molecular clock outlier\"]\n",
    "            if not any(i==l for i in mco):\n",
    "                mco.append(l)     \n",
    "\n",
    "\n",
    "    else:\n",
    "        if outlier not in to_remove:\n",
    "            for c, l in clusters.items():\n",
    "                if outlier in l:\n",
    "                    if not any(i==[outlier, \"molecular clock outlier\"] for i in mco):\n",
    "                        mco.append([outlier, \"molecular clock outlier\"])\n",
    "        else:\n",
    "            if not any(i==[outlier, \"molecular clock outlier\"] for i in mco):\n",
    "                mco.append([outlier, \"molecular clock outlier\"])         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update outlier file\n",
    "mco = pd.DataFrame.from_records(mco, columns=[\"Isolate_Id\",\"reason\"])\n",
    "mco.to_csv(molecular_clock_outliers, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
